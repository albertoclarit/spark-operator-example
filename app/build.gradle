/*
 * This file was generated by the Gradle 'init' task.
 *
 * This generated file contains a sample Java application project to get you started.
 * For more details on building Java & JVM projects, please refer to https://docs.gradle.org/8.13/userguide/building_java_projects.html in the Gradle documentation.
 */

plugins {
    // Apply the application plugin to add support for building a CLI application in Java.
    id 'application'
    // Apply the shadow plugin to generate a fat JAR
    id 'com.github.johnrengelman.shadow' version '8.1.1'
}

version = project.hasProperty('releaseVersion') ? project.releaseVersion : '0.1.0'

repositories {
    // Use Maven Central for resolving dependencies.
    mavenCentral()
}

dependencies {
    // Use JUnit Jupiter for testing.
    testImplementation 'org.junit.jupiter:junit-jupiter:5.9.1'

    testRuntimeOnly 'org.junit.platform:junit-platform-launcher'

    // This dependency is used by the application.
    implementation 'com.google.guava:guava:31.1-jre'

    // Apache Spark dependencies
    compileOnly 'org.apache.spark:spark-core_2.12:3.5.5'
    compileOnly 'org.apache.spark:spark-sql_2.12:3.5.5'

    // Hadoop AWS dependencies
    compileOnly 'org.apache.hadoop:hadoop-aws:3.3.4'
    compileOnly 'org.apache.hadoop:hadoop-common:3.3.4'
    compileOnly 'org.apache.hadoop:hadoop-client:3.3.4'
    compileOnly 'org.apache.hadoop:hadoop-hdfs:3.3.4'
    compileOnly 'org.apache.hadoop:hadoop-mapreduce-client-core:3.3.4'
    compileOnly 'com.amazonaws:aws-java-sdk-bundle:1.12.261'
}

// Apply a specific Java toolchain to ease working on different environments.
java {
    toolchain {
        languageVersion = JavaLanguageVersion.of(17)
    }
}

application {
    // Define the main class for the application.
    mainClass = 'org.example.App'
}

tasks.named('test') {
    // Use JUnit Platform for unit tests.
    useJUnitPlatform()
}

shadowJar {
    archiveBaseName.set('app')
    archiveClassifier.set('all')
    archiveVersion.set(version)
    mergeServiceFiles()
}

configurations {
    hadoopAwsDeps
}

dependencies {
    hadoopAwsDeps 'org.apache.hadoop:hadoop-aws:3.3.4'
    hadoopAwsDeps 'org.apache.hadoop:hadoop-common:3.3.4'
    hadoopAwsDeps 'org.apache.hadoop:hadoop-client:3.3.4'
  //  hadoopAwsDeps 'org.apache.hadoop:hadoop-hdfs:3.3.4'
    hadoopAwsDeps 'org.apache.hadoop:hadoop-mapreduce-client-core:3.3.4'
    hadoopAwsDeps 'com.amazonaws:aws-java-sdk-bundle:1.12.261'
}

task copyDependencies(type: Copy) {
    from configurations.hadoopAwsDeps
    into "${rootProject.projectDir}/libs"
}

abstract class UpdateSparkYaml extends DefaultTask {
    @InputDirectory
    abstract DirectoryProperty getLibsDir()

    @OutputFile
    abstract RegularFileProperty getYamlFile()

    @TaskAction
    def updateYaml() {
        def yamlContent = getYamlFile().get().asFile.text
        
        // Find the start of deps.jars section
        def depsStart = yamlContent.indexOf('  deps:')
        def jarsStart = yamlContent.indexOf('    jars:', depsStart)
        def jarsEnd = yamlContent.indexOf('  driver:', jarsStart)
        
        // Get all jar files from libs directory
        def jarFiles = getLibsDir().get().asFile.listFiles()
            .findAll { it.name.endsWith('.jar') }
            .collect { it.name }
            .sort()
        
        // Create new jars section
        def newJarsSection = '  deps:\n    jars:\n'
        jarFiles.each { jar ->
            newJarsSection += '      - "local:///opt/spark/work-dir/jars/' + jar + '"\n'
        }
        
        // Replace the old section with new one
        def newContent = yamlContent.substring(0, depsStart) + 
                        newJarsSection + 
                        yamlContent.substring(jarsEnd)
        
        getYamlFile().get().asFile.text = newContent
        
        println "Updated spark-application.yaml with ${jarFiles.size()} jars"
    }
}

tasks.register('updateSparkYaml', UpdateSparkYaml) {
    dependsOn copyDependencies
    libsDir = layout.projectDirectory.dir('../libs')
    yamlFile = layout.projectDirectory.file('../spark-application.yaml')
}

abstract class SetVersionTask extends DefaultTask {
    @Input
    @Optional
    abstract Property<String> getNewVersion()

    @Input
    @Optional
    abstract Property<String> getDockerRepository()

    @Internal
    abstract DirectoryProperty getProjectDir()

    @InputFile
    Provider<RegularFile> getInputPropertiesFile() {
        return projectDir.file('gradle.properties')
    }

    @OutputFile
    Provider<RegularFile> getOutputPropertiesFile() {
        return projectDir.file('gradle.properties')
    }

    @OutputFile
    Provider<RegularFile> getSparkApplicationFile() {
        return projectDir.file('../spark-application.yaml')
    }

    String incrementPatchVersion(String version) {
        def parts = version.tokenize('.')
        if (parts.size() != 3) return '0.0.1'
        
        def major = parts[0] as Integer
        def minor = parts[1] as Integer
        def patch = parts[2] as Integer
        
        return "${major}.${minor}.${patch + 1}"
    }

    void updateSparkApplicationImage(String repository, String version) {
        def sparkYamlFile = getSparkApplicationFile().get().asFile
        def lines = sparkYamlFile.readLines()
        
        def newLines = lines.collect { line ->
            if (line.trim().startsWith('image:')) {
                return "  image: \"${repository}:${version}\""
            }
            return line
        }
        
        sparkYamlFile.text = newLines.join('\n') + '\n'
        println "Updated spark-application.yaml image to: ${repository}:${version}"
    }

    @TaskAction
    def updateVersion() {
        def propsFile = getInputPropertiesFile().get().asFile
        def props = new Properties()
        
        if (propsFile.exists()) {
            propsFile.withInputStream { props.load(it) }
        }
        
        def currentVersion = props.getProperty('releaseVersion', '0.0.0')
        def version = getNewVersion().orNull ?: incrementPatchVersion(currentVersion)
        
        props.setProperty('releaseVersion', version)
        getOutputPropertiesFile().get().asFile.withOutputStream { props.store(it, null) }
        
        println "Updated version to: ${version}"

        // Update spark-application.yaml if repository is provided
        def repository = getDockerRepository().orNull
        if (repository) {
            updateSparkApplicationImage(repository, version)
        }
    }
}

tasks.register('setVersion', SetVersionTask) {
    newVersion = providers.gradleProperty('newVersion')
    dockerRepository = providers.environmentVariable('DOCKER_REPOSITORY')
    projectDir = layout.projectDirectory
}

abstract class DockerBuildPushTask extends DefaultTask {
    @Input
    abstract Property<String> getDockerRepository()

    @InputDirectory
    abstract DirectoryProperty getProjectDir()

    @InputFile
    Provider<RegularFile> getPropertiesFile() {
        return projectDir.file('gradle.properties')
    }

    String readVersion() {
        def props = new Properties()
        getPropertiesFile().get().asFile.withInputStream { props.load(it) }
        return props.getProperty('releaseVersion', '0.0.1')
    }

    @TaskAction
    def buildAndPush() {
        def repository = getDockerRepository().get()
        if (!repository) {
            throw new GradleException('DOCKER_REPOSITORY environment variable is required')
        }

        def version = readVersion()
        def imageTag = "${repository}:${version}"

        // Build the Docker image
        def buildCmd = ["docker", "build", "-t", imageTag, "."] as String[]
        def buildProc = new ProcessBuilder(buildCmd)
            .directory(projectDir.get().asFile.parentFile)
            .inheritIO()
            .start()
        
        if (buildProc.waitFor() != 0) {
            throw new GradleException('Docker build failed')
        }
        println "Built Docker image: ${imageTag}"

        // Push the Docker image
        def pushCmd = ["docker", "push", imageTag] as String[]
        def pushProc = new ProcessBuilder(pushCmd)
            .directory(projectDir.get().asFile.parentFile)
            .inheritIO()
            .start()
        
        if (pushProc.waitFor() != 0) {
            throw new GradleException('Docker push failed')
        }
        println "Pushed Docker image: ${imageTag}"
    }
}

tasks.register('dockerBuildPush', DockerBuildPushTask) {
    dependsOn tasks.shadowJar, tasks.copyDependencies
    dockerRepository = providers.environmentVariable('DOCKER_REPOSITORY')
    projectDir = layout.projectDirectory
}

tasks.register('buildAndDeploy') {
    dependsOn 'clean'
    dependsOn 'shadowJar'
    dependsOn 'copyDependencies'
    dependsOn 'updateSparkYaml'
    dependsOn 'setVersion'
    dependsOn 'dockerBuildPush'
    
    tasks.findByName('shadowJar').mustRunAfter 'clean'
    tasks.findByName('copyDependencies').mustRunAfter 'shadowJar'
    tasks.findByName('updateSparkYaml').mustRunAfter 'copyDependencies'
    tasks.findByName('setVersion').mustRunAfter 'updateSparkYaml'
    tasks.findByName('dockerBuildPush').mustRunAfter 'setVersion'
}
