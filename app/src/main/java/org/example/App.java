/*
 * This source file was generated by the Gradle 'init' task
 */
package org.example;

import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class App {
    public static void main(String[] args) {
        // Create Spark session
        SparkSession spark = SparkSession.builder()
                .appName("S3 JSON Reader")
               // .master("local[*]")
                .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider")
                .getOrCreate();

        try {
            // Read JSON data from S3
            Dataset<Row> df = spark.read()
                   // .option("header", "true")
                    .parquet("file:///opt/spark/work-dir/data/flights-1m.parquet");

            // Show schema
            System.out.println("Schema:");
            df.printSchema();

            // Show first few records
            System.out.println("\nSample Data:");
            df.show(5);

            // Print total number of records
            System.out.println("\nTotal number of records: " + df.count());

        } catch (Exception e) {
            System.err.println("Error reading from file: " + e.getMessage());
            e.printStackTrace();
        } finally {
            spark.close();
        }
    }
}
